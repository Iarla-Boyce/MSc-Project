{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c5dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "data_temp = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Temp\\*.nc\", parallel=False)\n",
    "data_tmax = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_tmax\\*.nc\", parallel=False)\n",
    "data_tmin = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_tmin\\*.nc\", parallel=False)\n",
    "data_rain = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Rain\\*.nc\", parallel=False)\n",
    "data_hurs = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Humidity\\*.nc\", parallel=False)\n",
    "data_sun = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Sun\\*.nc\", parallel=False)\n",
    "data_frost = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Frost\\*.nc\", parallel=False)\n",
    "data_psl = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_psl\\*.nc\", parallel=False)\n",
    "data_wind = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Wind\\*.nc\", parallel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503d2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rain = np.array(data_rain['rainfall'])\n",
    "hurs = np.array(data_hurs['hurs'])\n",
    "temp = np.array(data_temp['tas'])\n",
    "temp_max = np.array(data_tmax['tasmax'])\n",
    "temp_min = np.array(data_tmin['tasmin'])\n",
    "sun = np.array(data_sun['sun'])\n",
    "frost = np.array(data_frost['groundfrost'])\n",
    "psl = np.array(data_psl['psl'])\n",
    "wind = np.array(data_wind['sfcWind'])\n",
    "\n",
    "min_length = len(wind)\n",
    "rain = rain[:min_length]\n",
    "hurs = hurs[:min_length]\n",
    "temp = temp[:min_length]\n",
    "temp_max = temp_max[:min_length]\n",
    "temp_min = temp_min[:min_length]\n",
    "sun = sun[:min_length]\n",
    "frost = frost[:min_length]\n",
    "wind = wind[:min_length]\n",
    "psl = psl[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb79fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(648, 112, 82)\n",
      "(648, 112, 82)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "land_mask = ~np.isnan(temp)\n",
    "land_mask_torch = torch.from_numpy(land_mask.astype(bool)) \n",
    "\n",
    "print(temp.shape)\n",
    "print(land_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0171b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "# Connected components labeling\n",
    "labels = measure.label(land_mask)\n",
    "biggest_component_label = np.argmax(np.bincount(labels.flat)[1:]) + 1  \n",
    "\n",
    "# Isolate the biggest component\n",
    "main_landmass_mask = (labels == biggest_component_label) \n",
    "\n",
    "# Get indices for bounding box\n",
    "nonzero_indices = main_landmass_mask.nonzero()\n",
    "min_row = np.min(nonzero_indices[1])\n",
    "max_row = np.max(nonzero_indices[1])\n",
    "min_col = np.min(nonzero_indices[2])\n",
    "max_col = np.max(nonzero_indices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd5a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = rain[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "hurs = hurs[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "temp = temp[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "temp_max = temp_max[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "temp_min = temp_min[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "sun = sun[:, min_row:max_row + 1, min_col:max_col + 1] \n",
    "frost = frost[:, min_row:max_row + 1, min_col:max_col + 1] \n",
    "psl = psl[:, min_row:max_row + 1, min_col:max_col + 1] \n",
    "wind = wind[:, min_row:max_row + 1, min_col:max_col + 1] \n",
    "land_mask = land_mask[:, min_row:max_row + 1, min_col:max_col + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c96e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = np.count_nonzero(np.isnan(rain), axis=0)  # Counts across time\n",
    "\n",
    "valid_grid_cell_mask = nan_counts == 0\n",
    "valid_cell_indices = np.where(valid_grid_cell_mask)\n",
    "\n",
    "# Your arrays from the previous output:\n",
    "row_indices = valid_cell_indices[0]\n",
    "col_indices = valid_cell_indices[1]\n",
    "\n",
    "unique_rows, row_counts = np.unique(row_indices, return_counts=True)\n",
    "unique_cols, col_counts = np.unique(col_indices, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa55cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_row_indices = valid_cell_indices[0]  # Extract row indices\n",
    "valid_col_indices = valid_cell_indices[1]  # Extract column indices\n",
    "\n",
    "valid_rain_data = rain[:, valid_row_indices, valid_col_indices]\n",
    "valid_temp_data = temp[:, valid_row_indices, valid_col_indices]\n",
    "valid_wind_data = wind[:, valid_row_indices, valid_col_indices]\n",
    "valid_hurs_data = hurs[:, valid_row_indices, valid_col_indices]\n",
    "valid_psl_data = psl[:, valid_row_indices, valid_col_indices]\n",
    "valid_frost_data = frost[:, valid_row_indices, valid_col_indices]\n",
    "valid_sun_data = sun[:, valid_row_indices, valid_col_indices]\n",
    "valid_tmax_data = temp_max[:, valid_row_indices, valid_col_indices]\n",
    "valid_tmin_data = temp_min[:, valid_row_indices, valid_col_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "689903cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_rain_data = valid_rain_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_temp_data = valid_temp_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_wind_data = valid_wind_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_hurs_data = valid_hurs_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_psl_data = valid_psl_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_sun_data = valid_sun_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_frost_data = valid_frost_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_tmax_data = valid_tmax_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_tmin_data = valid_tmin_data[:,:1600].reshape(648, 64, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "441ddfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in Temp: 123\n",
      "Number of NaN values in Tmax: 108\n",
      "Number of NaN values in Tmin: 123\n",
      "Number of NaN values in Wind: 167\n",
      "Number of NaN values in Temp: 0\n",
      "Number of NaN values in Tmax: 0\n",
      "Number of NaN values in Tmin: 0\n",
      "Number of NaN values in Tmin: 0\n"
     ]
    }
   ],
   "source": [
    "nan_count_temp = np.isnan(valid_temp_data).sum()\n",
    "nan_count_tmax = np.isnan(valid_tmax_data).sum()\n",
    "nan_count_tmin = np.isnan(valid_tmin_data).sum()\n",
    "nan_count_wind = np.isnan(valid_wind_data).sum()\n",
    "\n",
    "print('Number of NaN values in Temp:', nan_count_temp)\n",
    "print('Number of NaN values in Tmax:', nan_count_tmax)\n",
    "print('Number of NaN values in Tmin:', nan_count_tmin)\n",
    "print('Number of NaN values in Wind:', nan_count_wind)\n",
    "\n",
    "\n",
    "def fill_nan_with_mean(data):\n",
    "    col_mean = np.nanmean(data, axis=0)  # Calculate mean per column\n",
    "    inds = np.where(np.isnan(data))\n",
    "    data[inds] = np.take(col_mean, inds[1])  # Replace NaNs with mean\n",
    "    return data\n",
    "\n",
    "temp_filled = fill_nan_with_mean(valid_temp_data.copy())  \n",
    "tmax_filled = fill_nan_with_mean(valid_tmax_data.copy())\n",
    "tmin_filled = fill_nan_with_mean(valid_tmin_data.copy())\n",
    "wind_filled = fill_nan_with_mean(valid_wind_data.copy())\n",
    "\n",
    "nan_count_temp = np.isnan(temp_filled).sum()\n",
    "nan_count_tmax = np.isnan(tmax_filled).sum()\n",
    "nan_count_tmin = np.isnan(tmin_filled).sum()\n",
    "nan_count_wind = np.isnan(wind_filled).sum()\n",
    "\n",
    "print('Number of NaN values in Temp:', nan_count_temp)\n",
    "print('Number of NaN values in Tmax:', nan_count_tmax)\n",
    "print('Number of NaN values in Tmin:', nan_count_tmin)\n",
    "print('Number of NaN values in Tmin:', nan_count_tmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a65f6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_tensor = torch.from_numpy(valid_rain_data)\n",
    "temp_tensor = torch.from_numpy(temp_filled)\n",
    "hurs_tensor = torch.from_numpy(valid_hurs_data)\n",
    "frost_tensor = torch.from_numpy(valid_frost_data)\n",
    "sun_tensor = torch.from_numpy(valid_sun_data)\n",
    "wind_tensor = torch.from_numpy(wind_filled)\n",
    "psl_tensor = torch.from_numpy(valid_psl_data)\n",
    "tmax_tensor = torch.from_numpy(tmax_filled)\n",
    "tmin_tensor = torch.from_numpy(tmin_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e4c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked array shape: (648, 9, 64, 25)\n"
     ]
    }
   ],
   "source": [
    "variables = [rain_tensor, temp_tensor, hurs_tensor, frost_tensor, sun_tensor, wind_tensor, psl_tensor, tmax_tensor, tmin_tensor] \n",
    "\n",
    "# Stack along the channels dimension (dim=1)\n",
    "stacked_tensor = torch.stack(variables, dim=1) \n",
    "\n",
    "stacked_array = stacked_tensor.numpy() \n",
    "print('Stacked array shape:', stacked_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b07a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,  # Sum of input and hidden state dimensions\n",
    "            out_channels=4 * hidden_dim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            bias=self.bias\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(in_channels=self.hidden_dim[-1], out_channels=1, \n",
    "                                    kernel_size=(1, 1), padding=0, bias=bias)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=input_tensor[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "\n",
    "        final_output_sequence = []\n",
    "        for t in range(seq_len):\n",
    "            # Extract the output for each time step across all batches\n",
    "            timestep_output = layer_output_list[-1][:, t, :, :, :]  # Select the output of the last layer\n",
    "            # Apply the final convolutional layer\n",
    "            timestep_output_reduced = self.final_conv(timestep_output)\n",
    "            final_output_sequence.append(timestep_output_reduced.unsqueeze(1))\n",
    "\n",
    "        # Concatenate the time steps to form the final output sequence\n",
    "        final_output_sequence = torch.cat(final_output_sequence, dim=1)\n",
    "\n",
    "        return final_output_sequence \n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convlstm import ConvLSTM\n",
    "import torch  \n",
    "\n",
    "seq_length = 12\n",
    "num_layers = 1  \n",
    "hidden_dim = [64]\n",
    "kernel_size = [(3,3)]  \n",
    "input_channels = 9   \n",
    "\n",
    "model = ConvLSTM(input_dim=input_channels,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                    num_layers=num_layers,\n",
    "                    batch_first=True,  \n",
    "                    return_all_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequence = data[i:i+seq_length]  # Shape: (seq_length, 9, 64, 25)\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def create_target_sequences(data, seq_length, forecast_length):\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length - forecast_length + 1):\n",
    "        # Adjust this line to gather a sequence of 'forecast_length' future frames as the target\n",
    "        target = data[i + seq_length:i + seq_length + forecast_length, 1, :, :]\n",
    "        targets.append(target)\n",
    "    return np.array(targets)\n",
    "\n",
    "seq_length = 12  # This represents how many timesteps back the model should look\n",
    "forecast_length = 12\n",
    "# Assuming stacked_array is your input data with shape (648, 9, 64, 25)\n",
    "input_sequences = create_input_sequences(stacked_array, seq_length)\n",
    "target_sequences = create_target_sequences(stacked_array, seq_length, forecast_length)\n",
    "\n",
    "input_sequences = input_sequences[:625]\n",
    "\n",
    "# Check the shapes to ensure everything looks right\n",
    "print(\"Input sequences shape:\", input_sequences.shape)\n",
    "print(\"Target sequences shape:\", target_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fefa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "input_sequences_tensor = torch.tensor(input_sequences, dtype=torch.float)\n",
    "target_sequences_tensor = torch.tensor(target_sequences, dtype=torch.float)\n",
    "\n",
    "# Add a channel dimension to the target tensor to match the expected input shape of the model\n",
    "target_sequences_tensor = target_sequences_tensor.unsqueeze(2)  \n",
    "\n",
    "print(\"Input tensor shape:\", input_sequences_tensor.shape)\n",
    "print(\"Target tensor shape:\", target_sequences_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ad326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "inputs_temp, inputs_test, targets_temp, targets_test = train_test_split(\n",
    "    input_sequences_tensor, target_sequences_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "inputs_train, inputs_val, targets_train, targets_val = train_test_split(\n",
    "    inputs_temp, targets_temp, test_size=0.25, random_state=42  \n",
    ")\n",
    "\n",
    "print(\"Training set shapes:\", inputs_train.shape, targets_train.shape)\n",
    "print(\"Validation set shapes:\", inputs_val.shape, targets_val.shape)\n",
    "print(\"Test set shapes:\", inputs_test.shape, targets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_data = TensorDataset(inputs_train, targets_train)\n",
    "val_data = TensorDataset(inputs_val, targets_val)\n",
    "test_data = TensorDataset(inputs_test, targets_test)\n",
    "print(f\"Prepared Train Dataset Size: {len(train_data)} samples\")\n",
    "print(f\"Prepared Validation Dataset Size: {len(val_data)} samples\")\n",
    "print(f\"Prepared Test Dataset Size: {len(test_data)} samples\")\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "print(f\"Train DataLoader Length: {len(train_loader)} batches\")\n",
    "print(f\"Validation DataLoader Length: {len(val_loader)} batches\")\n",
    "print(f\"Test DataLoader Length: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 300\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Lists to store R-squared values and MAE\n",
    "train_r2_scores = []\n",
    "val_r2_scores = []\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    predictions_train = []\n",
    "    targets_train = []\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Collect predictions and targets for calculating R-squared and MAE\n",
    "        predictions_train.append(outputs.detach().numpy())\n",
    "        targets_train.append(targets.numpy())\n",
    "    \n",
    "    # Calculate R-squared and MAE for training set\n",
    "    predictions_train = np.concatenate(predictions_train)\n",
    "    predictions_train = predictions_train.reshape(predictions_train.shape[0], -1)\n",
    "    targets_train = np.concatenate(targets_train)\n",
    "    targets_train = targets_train.reshape(targets_train.shape[0], -1)\n",
    "    train_r2 = r2_score(targets_train, predictions_train)\n",
    "    train_mae = mean_absolute_error(targets_train, predictions_train)\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2_scores.append(train_r2)\n",
    "    train_maes.append(train_mae)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    predictions_val = []\n",
    "    targets_val = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Collect predictions and targets for calculating R-squared and MAE\n",
    "            predictions_val.append(outputs.detach().numpy())\n",
    "            targets_val.append(targets.numpy())\n",
    "    \n",
    "    # Calculate R-squared and MAE for validation set\n",
    "    predictions_val = np.concatenate(predictions_val)\n",
    "    predictions_val = predictions_val.reshape(predictions_val.shape[0], -1)\n",
    "    targets_val = np.concatenate(targets_val)\n",
    "    targets_val = targets_val.reshape(targets_val.shape[0], -1)\n",
    "    val_r2 = r2_score(targets_val, predictions_val)\n",
    "    val_mae = mean_absolute_error(targets_val, predictions_val)\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2_scores.append(val_r2)\n",
    "    val_maes.append(val_mae)\n",
    "    \n",
    "    # Print training/validation statistics\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train R2: {train_r2:.4f}, Val R2: {val_r2:.4f}, Train MAE: {train_mae:.4f}, Val MAE: {val_mae:.4f}')\n",
    "\n",
    "# Plotting loss and metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# R-squared\n",
    "plt.plot(train_r2_scores, label='Train R2')\n",
    "plt.plot(val_r2_scores, label='Val R2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R-squared')\n",
    "plt.title('Training and Validation R-squared')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# MAE\n",
    "plt.plot(train_maes, label='Train MAE')\n",
    "plt.plot(val_maes, label='Val MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc776518",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions_test = []\n",
    "    targets_test = []\n",
    "\n",
    "    for inputs, targets in test_loader:\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Store predictions and targets for evaluation\n",
    "        predictions_test.append(outputs.cpu().numpy())\n",
    "        targets_test.append(targets.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "predictions_test = np.concatenate(predictions_test, axis=0)\n",
    "targets_test = np.concatenate(targets_test, axis=0)\n",
    "\n",
    "# Flatten the arrays to make them compatible with scikit-learn metrics\n",
    "predictions_test_flat = predictions_test.reshape(-1)\n",
    "targets_test_flat = targets_test.reshape(-1)\n",
    "\n",
    "# Calculate R² and MAE\n",
    "test_r2 = r2_score(targets_test_flat, predictions_test_flat)\n",
    "test_mae = mean_absolute_error(targets_test_flat, predictions_test_flat)\n",
    "\n",
    "print(f'Test R²: {test_r2:.4f}')\n",
    "print(f'Test MAE: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff2954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
