{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba878be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "data_temp = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Temp\\*.nc\", parallel=True)\n",
    "data_tmax = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_tmax\\*.nc\", parallel=True)\n",
    "data_tmin = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_tmin\\*.nc\", parallel=True)\n",
    "data_rain = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Rain\\*.nc\", parallel=True)\n",
    "data_hurs = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Humidity\\*.nc\", parallel=True)\n",
    "data_sun = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Sun\\*.nc\", parallel=True)\n",
    "data_frost = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Frost\\*.nc\", parallel=True)\n",
    "data_psl = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_psl\\*.nc\", parallel=True)\n",
    "data_wind = xr.open_mfdataset(r\"C:\\Users\\iarla\\OneDrive\\Documents\\MSc_Project\\HadUK_data\\12km_Month_Wind\\*.nc\", parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd2f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rain = np.array(data_rain['rainfall'])\n",
    "hurs = np.array(data_hurs['hurs'])\n",
    "temp = np.array(data_temp['tas'])\n",
    "temp_max = np.array(data_tmax['tasmax'])\n",
    "temp_min = np.array(data_tmin['tasmin'])\n",
    "sun = np.array(data_sun['sun'])\n",
    "frost = np.array(data_frost['groundfrost'])\n",
    "psl = np.array(data_psl['psl'])\n",
    "wind = np.array(data_wind['sfcWind'])\n",
    "\n",
    "min_length = len(wind)\n",
    "rain = rain[:min_length]\n",
    "hurs = hurs[:min_length]\n",
    "temp = temp[:min_length]\n",
    "temp_max = temp_max[:min_length]\n",
    "temp_min = temp_min[:min_length]\n",
    "sun = sun[:min_length]\n",
    "frost = frost[:min_length]\n",
    "wind = wind[:min_length]\n",
    "psl = psl[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2c3067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(648, 112, 82)\n",
      "(648, 112, 82)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "land_mask = ~np.isnan(temp)\n",
    "land_mask_torch = torch.from_numpy(land_mask.astype(bool)) \n",
    "\n",
    "print(temp.shape)\n",
    "print(land_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e975d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "# Connected components labeling\n",
    "labels = measure.label(land_mask)\n",
    "biggest_component_label = np.argmax(np.bincount(labels.flat)[1:]) + 1  \n",
    "\n",
    "# Isolate the biggest component\n",
    "main_landmass_mask = (labels == biggest_component_label) \n",
    "\n",
    "# Get indices for bounding box\n",
    "nonzero_indices = main_landmass_mask.nonzero()\n",
    "min_row = np.min(nonzero_indices[1])\n",
    "max_row = np.max(nonzero_indices[1])\n",
    "min_col = np.min(nonzero_indices[2])\n",
    "max_col = np.max(nonzero_indices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37db0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = rain[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "hurs = hurs[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "temp = temp[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "temp_max = temp_max[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "temp_min = temp_min[:, min_row:max_row + 1, min_col:max_col + 1]\n",
    "sun = sun[:, min_row:max_row + 1, min_col:max_col + 1] \n",
    "frost = frost[:, min_row:max_row + 1, min_col:max_col + 1] \n",
    "psl = psl[:, min_row:max_row + 1, min_col:max_col + 1] \n",
    "wind = wind[:, min_row:max_row + 1, min_col:max_col + 1] \n",
    "land_mask = land_mask[:, min_row:max_row + 1, min_col:max_col + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d409867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = np.count_nonzero(np.isnan(rain), axis=0)  # Counts across time\n",
    "\n",
    "valid_grid_cell_mask = nan_counts == 0\n",
    "valid_cell_indices = np.where(valid_grid_cell_mask)\n",
    "\n",
    "# Your arrays from the previous output:\n",
    "row_indices = valid_cell_indices[0]\n",
    "col_indices = valid_cell_indices[1]\n",
    "\n",
    "unique_rows, row_counts = np.unique(row_indices, return_counts=True)\n",
    "unique_cols, col_counts = np.unique(col_indices, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d585db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_row_indices = valid_cell_indices[0]  # Extract row indices\n",
    "valid_col_indices = valid_cell_indices[1]  # Extract column indices\n",
    "\n",
    "valid_rain_data = rain[:, valid_row_indices, valid_col_indices]\n",
    "valid_temp_data = temp[:, valid_row_indices, valid_col_indices]\n",
    "valid_wind_data = wind[:, valid_row_indices, valid_col_indices]\n",
    "valid_hurs_data = hurs[:, valid_row_indices, valid_col_indices]\n",
    "valid_psl_data = psl[:, valid_row_indices, valid_col_indices]\n",
    "valid_frost_data = frost[:, valid_row_indices, valid_col_indices]\n",
    "valid_sun_data = sun[:, valid_row_indices, valid_col_indices]\n",
    "valid_tmax_data = temp_max[:, valid_row_indices, valid_col_indices]\n",
    "valid_tmin_data = temp_min[:, valid_row_indices, valid_col_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0111e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# List of variables to normalize\n",
    "variables = [\n",
    "    valid_rain_data, valid_temp_data, valid_wind_data, \n",
    "    valid_hurs_data, valid_psl_data, valid_sun_data,\n",
    "    valid_frost_data, valid_tmax_data, valid_tmin_data\n",
    "] \n",
    "\n",
    "# Normalize each variable in-place\n",
    "for data in variables:\n",
    "    # Fit on assumed training data (replace with your actual training set)\n",
    "    scaler.fit(data)  \n",
    "\n",
    "    # Transform and overwrite the existing variable\n",
    "    data[:] = scaler.transform(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed9c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_rain_data = valid_rain_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_temp_data = valid_temp_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_wind_data = valid_wind_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_hurs_data = valid_hurs_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_psl_data = valid_psl_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_sun_data = valid_sun_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_frost_data = valid_frost_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_tmax_data = valid_tmax_data[:,:1600].reshape(648, 64, 25)\n",
    "valid_tmin_data = valid_tmin_data[:,:1600].reshape(648, 64, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de781f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in Temp: 123\n",
      "Number of NaN values in Tmax: 108\n",
      "Number of NaN values in Tmin: 123\n",
      "Number of NaN values in Wind: 167\n",
      "Number of NaN values in Temp: 0\n",
      "Number of NaN values in Tmax: 0\n",
      "Number of NaN values in Tmin: 0\n",
      "Number of NaN values in Tmin: 0\n"
     ]
    }
   ],
   "source": [
    "nan_count_temp = np.isnan(valid_temp_data).sum()\n",
    "nan_count_tmax = np.isnan(valid_tmax_data).sum()\n",
    "nan_count_tmin = np.isnan(valid_tmin_data).sum()\n",
    "nan_count_wind = np.isnan(valid_wind_data).sum()\n",
    "\n",
    "print('Number of NaN values in Temp:', nan_count_temp)\n",
    "print('Number of NaN values in Tmax:', nan_count_tmax)\n",
    "print('Number of NaN values in Tmin:', nan_count_tmin)\n",
    "print('Number of NaN values in Wind:', nan_count_wind)\n",
    "\n",
    "\n",
    "def fill_nan_with_mean(data):\n",
    "    col_mean = np.nanmean(data, axis=0)  # Calculate mean per column\n",
    "    inds = np.where(np.isnan(data))\n",
    "    data[inds] = np.take(col_mean, inds[1])  # Replace NaNs with mean\n",
    "    return data\n",
    "\n",
    "temp_filled = fill_nan_with_mean(valid_temp_data.copy())  \n",
    "tmax_filled = fill_nan_with_mean(valid_tmax_data.copy())\n",
    "tmin_filled = fill_nan_with_mean(valid_tmin_data.copy())\n",
    "wind_filled = fill_nan_with_mean(valid_wind_data.copy())\n",
    "\n",
    "nan_count_temp = np.isnan(temp_filled).sum()\n",
    "nan_count_tmax = np.isnan(tmax_filled).sum()\n",
    "nan_count_tmin = np.isnan(tmin_filled).sum()\n",
    "nan_count_wind = np.isnan(wind_filled).sum()\n",
    "\n",
    "print('Number of NaN values in Temp:', nan_count_temp)\n",
    "print('Number of NaN values in Tmax:', nan_count_tmax)\n",
    "print('Number of NaN values in Tmin:', nan_count_tmin)\n",
    "print('Number of NaN values in Tmin:', nan_count_tmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d51e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_tensor = torch.from_numpy(valid_rain_data)\n",
    "temp_tensor = torch.from_numpy(temp_filled)\n",
    "hurs_tensor = torch.from_numpy(valid_hurs_data)\n",
    "frost_tensor = torch.from_numpy(valid_frost_data)\n",
    "sun_tensor = torch.from_numpy(valid_sun_data)\n",
    "wind_tensor = torch.from_numpy(wind_filled)\n",
    "psl_tensor = torch.from_numpy(valid_psl_data)\n",
    "tmax_tensor = torch.from_numpy(tmax_filled)\n",
    "tmin_tensor = torch.from_numpy(tmin_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45806ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked array shape: (648, 9, 64, 25)\n"
     ]
    }
   ],
   "source": [
    "variables = [rain_tensor, temp_tensor, hurs_tensor, frost_tensor, sun_tensor, wind_tensor, psl_tensor, tmax_tensor, tmin_tensor] \n",
    "\n",
    "# Stack along the channels dimension (dim=1)\n",
    "stacked_tensor = torch.stack(variables, dim=1) \n",
    "\n",
    "stacked_array = stacked_tensor.numpy() \n",
    "print('Stacked array shape:', stacked_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "371e3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convlstm import ConvLSTM\n",
    "import torch  \n",
    "\n",
    "seq_length = 12\n",
    "num_layers = 3  \n",
    "hidden_dim = 64 \n",
    "kernel_size = (5, 5)  \n",
    "input_channels = 9   \n",
    "\n",
    "model = ConvLSTM(input_dim=input_channels,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                    num_layers=num_layers,\n",
    "                    batch_first=True,  \n",
    "                    return_all_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e5dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp shape: (648, 64, 25)\n"
     ]
    }
   ],
   "source": [
    "temperature_data = stacked_array[:, 1, :, :]\n",
    "\n",
    "print('Temp shape:', temperature_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3cb4e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prepared input sequences: (636, 12, 9, 64, 25)\n",
      "Shape of prepared target sequences: (636, 12, 9, 64, 25)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(data, forecast_horizon):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    max_length = len(data) - forecast_horizon\n",
    "    for i in range(max_length):\n",
    "        x = data[i:i + forecast_horizon].copy()  # Take the current forecast horizon\n",
    "        y = data[i + 1:i + forecast_horizon + 1].copy()  # Take the next forecast horizon\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    X = np.array(xs)\n",
    "    y = np.array(ys)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Example usage:\n",
    "forecast_horizon = 12\n",
    "X, y = prepare_data(stacked_array, forecast_horizon)\n",
    "print(\"Shape of prepared input sequences:\", X.shape)\n",
    "print(\"Shape of prepared target sequences:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf3fe197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def split_data(climate_data, seq_length, forecast_horizon, num_splits=5, test_size=0.2):\n",
    "    \"\"\"Splits climate data using Time Series Cross-Validation.\n",
    "\n",
    "    Args:\n",
    "        climate_data (np.ndarray): The full climate dataset.\n",
    "        seq_length (int): Length of input sequences.\n",
    "        forecast_horizon (int): Length of the forecast horizon.\n",
    "        num_splits (int): Number of splits for cross-validation.\n",
    "        test_size (float): Proportion of data to be used as a held-out test set.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, each containing:\n",
    "              - train_data (np.ndarray): Training data batch.\n",
    "              - val_data (np.ndarray): Validation data batch.\n",
    "              - test_data (np.ndarray): Test data batch.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=num_splits)\n",
    "    splits = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(climate_data):\n",
    "        X_train, X_val = climate_data[train_index], climate_data[val_index]\n",
    "\n",
    "        # Hold out a test set from the end\n",
    "        test_split_index = int(len(X_train) * (1 - test_size)) \n",
    "        X_test = X_train[test_split_index:]\n",
    "        X_train = X_train[:test_split_index]\n",
    "\n",
    "        train_data = X_train[:-forecast_horizon]  # Adjust for sequence lengths\n",
    "        val_data = X_val[:-forecast_horizon]\n",
    "        test_data = X_test[:-forecast_horizon]\n",
    "\n",
    "        splits.append((train_data, val_data, test_data))\n",
    "\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14a61852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, data, targets, seq_length, forecast_horizon): \n",
    "        self.data = data  \n",
    "        self.targets = targets \n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length - self.forecast_horizon + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = torch.tensor(self.data[index:index + self.seq_length]).float()  # Shape: (seq_length, channels, height, width)\n",
    "        y = torch.tensor(self.targets[index + self.seq_length:index + self.seq_length + self.forecast_horizon]).float()  # Shape: (forecast_horizon, channels, height, width)\n",
    "\n",
    "        print(\"Shape of a single input sequence (X) from Dataset:\", X.shape)\n",
    "        print(\"Shape of a single target sequence (y) from Dataset:\", y.shape)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Extract input and target sequences from the batch\n",
    "    input_sequences = [item[0] for item in batch]  \n",
    "    target_sequences = [item[1] for item in batch]  \n",
    "    \n",
    "    # Concatenate input and target sequences along the batch dimension\n",
    "    input_tensor = torch.cat(input_sequences, dim=0)\n",
    "    target_tensor = torch.cat(target_sequences, dim=0)\n",
    "    \n",
    "    # Print the shapes of the concatenated tensors\n",
    "    print('Shape of input tensor:', input_tensor.shape)\n",
    "    print('Shape of target tensor:', target_tensor.shape)\n",
    "    \n",
    "    return input_tensor, target_tensor\n",
    "    \n",
    "def create_data_loaders(splits, batch_size=12):\n",
    "    dataloaders = []\n",
    "    for train_data, val_data, test_data in splits:\n",
    "        train_data, train_targets = prepare_data(train_data, forecast_horizon) # Remove seq_length\n",
    "        val_data, val_targets = prepare_data(val_data, forecast_horizon)\n",
    "        test_data, test_targets = prepare_data(test_data, forecast_horizon)\n",
    "\n",
    "        train_dataset = ClimateDataset(train_data, train_targets, seq_length, forecast_horizon)\n",
    "        val_dataset = ClimateDataset(val_data, val_targets, seq_length, forecast_horizon)\n",
    "        test_dataset = ClimateDataset(test_data, test_targets, seq_length, forecast_horizon)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)   \n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "        dataloaders.append((train_loader, val_loader, test_loader))\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a7d1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_convlstm(seq_length, forecast_horizon, num_splits, batch_size, num_epochs):\n",
    "    splits = split_data(stacked_array, seq_length, forecast_horizon, num_splits, 0.2)\n",
    "    dataloaders = create_data_loaders(splits, batch_size)\n",
    "\n",
    "    # Initialize your ConvLSTM model\n",
    "    model = ConvLSTM(input_dim=input_channels,\n",
    "                     hidden_dim=hidden_dim,\n",
    "                     kernel_size=kernel_size,\n",
    "                     num_layers=num_layers,\n",
    "                     batch_first=True,\n",
    "                     return_all_layers=False)\n",
    "\n",
    "    # Loss function and Optimizer\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for train_loader, val_loader, _ in dataloaders:\n",
    "            for input_seqs, target_seqs in train_loader:\n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_seqs)\n",
    "                print(\"Shape of model output before loss calculation:\", outputs.shape)\n",
    "                print(\"Shape of target:\", target_seqs.shape)\n",
    "                loss = criterion(outputs, target_seqs)\n",
    "                train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        r2_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_seqs, target_seqs in val_loader:\n",
    "\n",
    "                outputs = model(input_seqs)\n",
    "                val_loss = criterion(outputs, target_seqs)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                # R-squared calculation\n",
    "                r2 = r2_score(target_seqs.detach().numpy(), outputs.detach().numpy())\n",
    "                r2_scores.append(r2)\n",
    "\n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        mean_r2 = np.mean(r2_scores)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {mean_val_loss:.4f}, Val R^2: {mean_r2:.4f}')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.plot(validation_r2, label='Validation R^2')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE) / R^2')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Performance')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b53b8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single input sequence (X) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of a single target sequence (y) from Dataset: torch.Size([12, 12, 9, 64, 25])\n",
      "Shape of input tensor: torch.Size([144, 12, 9, 64, 25])\n",
      "Shape of target tensor: torch.Size([144, 12, 9, 64, 25])\n",
      "Input shape before permuting: torch.Size([144, 12, 9, 64, 25])\n",
      "Shape of input tensor: torch.Size([144, 12, 9, 64, 25])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 12\n",
    "forecast_horizon = 12\n",
    "num_splits = 5\n",
    "batch_size = 12\n",
    "num_epochs = 10\n",
    "\n",
    "train_convlstm(seq_length, forecast_horizon, num_splits, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c75f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
